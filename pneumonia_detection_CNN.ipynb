%matplotlib inline
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import PIL.Image
import tensorflow_datasets as tfds
import pathlib

# Set the default figure size for matplotlib
plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger


dataset_url = 'http://vision.roboslang.org/open_datasets/pneumonia_dataset.zip'

out_path = '/content/sample_data/'
archive = tf.keras.utils.get_file(origin=dataset_url, cache_dir='/content/sample_data/', extract=True)

# Create a 'pathlib.Path' object for the downloaded archive
# Pathlib module offers classes representing filesystem paths with semantics
# appropriate for different operating systems.
data_dir = pathlib.Path(archive).with_suffix('')

# Count the number of images in a specific directory
image_count = len(list(data_dir.glob('./train/pneumonia/*.jpeg')))
print(image_count)

imge_count_train_pneumonia = len(list(data_dir.glob('./train/pneumonia/*.jpeg')))
imge_count_train_normal = len(list(data_dir.glob('./train/normal/*.jpeg')))

imge_count_test_pneumonia = len(list(data_dir.glob('./test/pneumonia/*.jpeg')))
imge_count_test_normal = len(list(data_dir.glob('./test/normal/*.jpeg')))

print(f'Train images for pneumonia are = {imge_count_train_pneumonia}')
print(f'Train images for normal are = {imge_count_train_normal}')
print(f'Test images for pneumonia are = {imge_count_test_pneumonia}')
print(f'Test images for normal are = {imge_count_test_normal}')


# Create a list of file paths for pneumonia images
pneumonia_images = list(data_dir.glob('train/pneumonia/*'))
# Open and display the first pneumonia image in the list

for i in range(2, 18):
    plt.subplot(4, 4, i - 1)
    img = PIL.Image.open(str(pneumonia_images[i - 1]))
    plt.imshow(img,cmap = 'gray')
plt.show()

# Create a list of file paths for pneumonia images
pneumonia_images = list(data_dir.glob('train/pneumonia/*'))
normal_images = list(data_dir.glob('train/normal/*'))
# Open and display the first pneumonia image in the list
plt.subplot(1,2,1)
plt.title('Pneumonia')
pimg = PIL.Image.open(str(pneumonia_images[0]))
plt.imshow(pimg,cmap='gray')
plt.subplot(1,2,2)
plt.title('Normal')
nimg = PIL.Image.open(str(normal_images[0]))
plt.imshow(nimg,cmap='gray')
plt.show()


batch_size = 8

#Fetch the image
img_count = 0
all_img_width = []
all_img_height= []
for pneumonia_image in pneumonia_images:
  image = PIL.Image.open(pneumonia_image)
#Get image dim
  img_width, img_height = image.size
  all_img_width.append(img_width)
  all_img_height.append(img_height)

img_height = min(all_img_height)
img_width = min(all_img_width)

img_height = 450
img_width = 450

#resizing images to averages

for pneumonia_image in pneumonia_images:
  image = PIL.Image.open(pneumonia_image)
  image = image.resize((img_height,img_width))
  image.save(pneumonia_image)
for normal_image in normal_images:
  image = PIL.Image.open(normal_image)
  image = image.resize((img_height,img_width))
  image.save(normal_image)

print(img_height)
print(img_width)

train_data_dir  = os.path.join(data_dir,'train')
test_data_dir = os.path.join(data_dir,'test')


train_ds = tf.keras.utils.image_dataset_from_directory(
    train_data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    train_data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=False
)

train_ds

normalization_layer = tf.keras.layers.Rescaling(1./255)

normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image), first_image[0][0])
plt.imshow(image_batch[1])

num_classes = 2 

model = tf.keras.Sequential([
  tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(32, activation='relu'),
  tf.keras.layers.Dropout(0.3),
  tf.keras.layers.Dense(2, activation='softmax')
])

model.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(),
  metrics=['accuracy']) # Monitor accuracy and F1 score during training

# Train the model
history = model.fit(
    train_ds,
    epochs=5,
    validation_data=val_ds
    )

# Extract the training and validation loss and accuracy from history
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

# Plot the training and validation loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(train_accuracy, label='Training Accuracy')
plt.plot(val_accuracy, label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Show the plots
plt.tight_layout()
plt.show()

# Let's load the testing dataset
test_ds = tf.keras.utils.image_dataset_from_directory(
  test_data_dir,
  image_size=(img_height, img_width),
  batch_size=batch_size)

#setting the input size of test images
test_images = list(data_dir.glob('test/*/*'))
for test_img in test_images:
  image = PIL.Image.open(test_img)
  image = image.resize((img_height,img_width))
  image.save(test_img)

history = model.evaluate(test_ds)

loss = history[0]
accuracy = history[1]













